## 介绍

ResNet, Hightway Networks。

为了确保网络各层之间的信息流最大化，可以将所有和特征图大小匹配的层直接连接，确保从前面的层获取额外的输入，并将自身的特征图传给后续层。

DenseNet比传统卷积网络需要更少的参数，因为不需要那么多的通道数。

ResNet表明许多层的贡献非常小，并且许多参数是冗余的，在训练期间可以随机丢弃。

**DenseNet的主要特点**
- 所有层的输出通过拼接的方式传到后续的层，使得每层都能访问前面层的特征图，共享集体知识。
- 特征图数量少，参数量不大，计算效率高，并且可以有效防止过拟合。

## 密集连接

卷积网络由若干非线性卷积块组成，每个卷积块表示一个非线性转换$H_l$。

### ResNet

$$x_l = H_l(x_{l-1})+x_{l-1}$$

ResNet可以让梯度更顺利的流到后续的层中，但求和会阻碍网络中信息的流动。

### 密集连接

为了提示信息流动性，可以添加从任何层到后续层的直接连接。

$$x_l = H_l([x_0,x_1,...,x_{l-1}])$$

称为DenseNet，连接过程是直接将输出特征图和前面层的特征图拼接在一起。

### 复合函数

每个卷积块中包含卷积层，非线性激活层和归一化层。

### 池化层

拼接操作存在特征尺度不一致的问题，浅层网络特征尺度较大而深层网络特征尺度较小。可以用池化层进行下采样来保持特征图尺度一致。

**密集块**：将网络分为多个密集块，密集连接只在密集块之内，密集块之间通过一个过渡层连接，过渡层中包含归一化层，$1\times 1$卷积层和一个步长为2的池化层组成。

### 增长率

如果每个函数$H_l$产生$k$个特征图，$l_{th}$层就有$k_0+k\times(l-1)$个特征图。（实验显示）较小的增长率就可以获得最好的结果，因为每一层都可以访问先前所有特征图（集体知识）。

可以将特征图视为全局状态，每层都会将当前层的输出特征图加入到全局状态中，后面的层可以直接访问全局状态中的状态。

### 瓶颈层

在每个$3\times 3$卷积层前引入$1\times 1$的瓶颈层来减少网络中的通道数量并提高计算效率。

### 压缩

为了提高模型紧凑性，可以减少过渡层中特征图的数量。如果一个密集块包含$m$个特征图，可以让后续过渡层生成$\theta m$个输出特征图，$\theta$称为压缩因子。

## 实验

## 讨论

和ResNet主要不同在于连接是拼接而非求和。

### 模型紧凑性

如何DenseNet层学到的特征图都可以被后续层访问，鼓励了整个网络中特征重用。

隐式深度监督