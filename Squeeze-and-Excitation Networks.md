## 引言

在CNN中引入新的模块，称为Squeeze-and-Excitation，目标是**显式建模卷积特征各个通道之间的依赖关系**。这种机制可以学习全局信息并对网络进行特征重新校准，来选择性的强调关键信息并抑制不重要的信息。

首先对特征$U$执行Squeeze操作，该操作通过**聚合整个空间维度的特征**来生成通道描述符。这个描述符表示一个通道级别的全局分布嵌入，包含了全部感受野的信息供后面的层使用。聚合层后是Excitation操作，采用自门控机制来将**输入的描述符生成每个通道的调节权重**。最终这个权重被应用于特征图$U$来产生SE模块的输出。

虽然SE模块是通用的，但在不同层中作用不同。在浅层中，它以与类别无关的方式激发信息特征，在深层中，SE会变得专业化，以和类别高度相关的方式响应输入。最终SE的特征重校准将使整个网络受益。

SE模块设计简单，计算轻量。

## 相关工作

总结了CNN历史，VGGNet，Inception，ResNet。

### 注意力和门控机制

注意力机制可以解释为**将计算资源偏向信号中信息量最大的成分**。

## SE模块

SE模块可以构建在一个将输入$X$映射到输出$U$的卷积变换$F_{tr}$。记$V$为的一组卷积核，输出$u_c = v_c * X$,其中$*$是卷积操作。

因为输出是所有通道求和得到的，通道间的依赖关系已经隐含的包括在输出特征图中了，但是由于卷积运算的性质，这种依赖关系是**局部的和隐式的**。

我们的目标是通过显示建模通道间的互相依赖关系来增强卷积特征的学习，以便网络能够提高其对信息特征的敏感性。让特征访问**全局信息**，通过两步重新校准滤波器响应。

### Squeeze

因为卷积核工作在局部感受野上，每个输出都无法利用上下文信息。通过全局平均池化来将全局信息压缩到一个通道描述符$z_c$中。
$$z_c = \frac{1}{H\times W} \sum_{i=1}^H \sum_{j=1}^W u_c(i,j)$$

### Excitation

首先需要捕获通道间的依赖性，使用带有sigmoid的门控机制：
$$s = F_{ex}(\vec z,W) = \sigma(g(\vec z,W)) = \sigma(W_2 \sigma(W_1 \vec z))$$

为了限制模型的复杂性以提升泛化能力，在非线性层周围添加两个全连接层实现一个瓶颈层，非线性层进行维度缩减，记缩减率为$r$。然后是一个返回变换输出的增维层。最后通过缩放$U$来获得输出。

讨论：Excitation操作将$\vec z$映射为通道权重，从这方面讲，SE模块本质上引入了基于输入的动态性，可以视为通道上的自注意力函数，这种自注意力不受限于卷积核的局部感受野。

### 特征重新校准

$$\tilde U = s \odot U$$

### 实例化

SE模块可以直接集成在传统CNN的非线性层之后。

在Inception网络中，将转换$F_{tr}$加在Inception模块之后，得到了SE-Inception网络。在ResNet网络中，在残差相加之前加入SE模块的得到SE-ResNet。

## 计算复杂性

在ResNet-50和SE-ResNet-50之间比较，当$r$设置为16时，比原始ResNet-50的GFLOPs增加这里0.26%，精度接近ResNet-101。

额外参数主要由两个FC层产生，引入参数数量为：
$$\frac{2}{r}\sum_{s=1}^S N_S \cdot C_S^2$$

## 实验

动量0.9，批量1024，学习率0.6，每30个epoch降低10倍，r(缩减率)=16。

## 消融研究

### 缩减率

性能不会随着复杂性增加而一直增加，较小的缩减率反而会显著增加模型参数大小，文章中的实验是16最好。

### Squeeze

全局平均比全局最大好点，但选不同的聚合操作差异不大。

### Excitation

用Tanh会稍微降低准确率，但换成ReLU会显著降低准确率。

### 集成策略

以ResNet为例，只要在残差连接之前，差距都不大，但放到残差连接后，准确率显著下降。

