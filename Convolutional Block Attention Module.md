## 引言

在CNN中，相比网络的深度和宽度，势（cardinality）既能节省参数，又有更强的表示能力。

注意力机制的目标是提高特征的表示能力：关注重要特征，抑制不重要特征。卷积层会将跨通道的信息和空间信息进行混合。

> 卷积计算会将前一层所有通道对应卷积核感受野的信息压缩在一起。

我们的模块将沿着两个主要维度来强调重要特征：通道和空间位置。

## 相关工作

### 网络工程

增加网络深度和宽度（AlexNet，VGGNet）

残差连接（ResNet）

分组卷积，增加势（ResNeXt，Inception）

DenseNet

### 注意力机制

我们不直接计算3D（特征图长，宽以及通道数量）注意力图，而是分解为通道注意力和空间注意力，这样参数会少很多。

> 分解后参数数量从$c*h*w$变为$c+h*w$。

SENet使用了全局平局池化，本文作者觉得最大池化也很重要，并且SENet没有关注空间注意力，CBAM中将同时利用通道注意力和空间注意力。

### 卷积块注意力模块

给中间特征图F，CBMA会按顺序算出一个1D通道注意力图和一个2D空间注意力图，并利用注意力图调节特征图F。

$$F'=M_c(F) \otimes F $$
$$
F'' = M_s(F') \otimes F'
$$
$\otimes$表示元素相乘。

#### 通道注意力模块

利用特征的通道间关系生成通道注意力图。之前的文章有的认为平均池化更好，有的认为最大池化更好。作者这里同时使用了这两个特征。

首先利用平均池化和最大池化聚合空间信息，生成两个上下文描述符$F_cavg$和$F_cmax$，然后依次进入一个共享的MLP来产生通道注意力图。和SENet一样，共享MLP包含了一个用于缩减特征的隐层，特征缩减率$r$。最后利用注意力图对原特征图进行缩放。

#### 空间注意力模块

分别沿通道轴应用平均池化和最大池化
$$
z_{Max}^{i,j} = Max(F_0(i,j),...,F_c(i,j))
$$$$
z_{Avg}^{i,j}=\frac{1}{c} \sum_{i=0}^{c}F_c(i,j)
$$
并将他们串联起来生成$2\times H \times W$的特征描述符。在该串联的特征描述符上应用卷积生成空间注意力图。该图突出了空间上需要强调或者抑制的区域。

#### 注意力模块的安排

通道注意力模块和空间注意力模块间可以并联或串联，但实验结果发现串联的效果更好。

## 实验

### 消融研究

#### 通道注意力

比较了
- 单独使用最大池化
- 单独使用平均池化
- 两者都使用

MLP的缩减率16。发现最大池化也有独特的意义，因此建议两者都使用。

#### 空间注意力

显示的建模池化可以产生更精细的注意力推理。

#### 通道和空间注意力的安排

比较了顺序的排列和并行的排列，并行排列拥有更好的效率，但顺序排列产生的注意力图更加精细，并且优先通道注意力拥有更好的性能。因此最终采用串联模式，并且通道注意力在前。

## 实验

利用Grad-CAM可视化结果，Grad-CAM可以更好的覆盖目标区域。