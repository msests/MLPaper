## 2 网络配置
### 2.1 & 2.2 配置和架构

预处理：每个像素减去训练集上的像素值均值。

卷积块：
- 数个感受野为$3 \times 3$，步长为$1$的卷积层。某些配置中还会使用$1 \times 1$的卷积大小来增加非线性。
- 池化层：最大池化，$2 \times 2$，步长为$2$。
- 激活层：ReLU

全连接层：三层，前两层有$4096$个通道，最后一个有$1000$个通道。最后用softmax层。

每个卷积块都相对上一个卷积块尺寸减半，通道加倍。不同配置的网络中卷积块中包含的卷积层数量不一样。

![](../images/VGGNetConfig.png)

### 2.3 讨论

和之前的网络中大感受野不同，VGG使用较小的$3 \times 3$感受野，而三层$3 \times 3$的感受野堆叠效果相当于一层$7 \times 7$。这种做法增加了网络捕捉特征的能力（可以关注更小的区域），并且减少了网络参数的数量。

使用$1 \times 1$的卷积可以在不改变感受野的情况下增加网络的非线性能力。在”网络中的网络“里使用。

## 3 框架

### 3.1 训练

256 mini-batch；动量系数$0.9$；L2正则化权重衰减，惩罚系数$10^{-4}$；前两个全连接层$0.5$的dropout率；学习率$10^{-2}$，每当验证集不再变化时减小$10$倍。

使用均值为$0$，方差$10^{-2}$的正则分布对参数进行初始化。

> 我复现时用这种方式初始化，损失从一开始就几乎不降，应该是严重的梯度消失。改用He初始化就正常了。

先训练浅网络，并用浅层网络的参数初始化深网络的权重。（主要是为了防止参数分布的不稳定，那时还没有各种Normalization技术。）

输入到网络的图像使用固定的$224 \times 224$大小图像，是从等比例缩放过的训练图像中随机裁剪得到。可以通过随机的水平翻转和颜色偏移来扩充训练集。

假设S为预设的训练图像等比例缩放后的最小边长，可以令S固定为$256$或$384$，或者从$[256，512]$中随机选择，并从其中随机裁剪$224 \times 224$。实验结果显示随机边长会带来更鲁棒的网络。

### 3.2 测试

首先等比例缩放至最小边长Q（不一定等于S），

### 3.3 实现细节

多GPU，每批图像分成几组交给不同GPU，GPU同时计算梯度，并将所有梯度的平均值作为批的梯度。

## 4 分类实验

在ILSVRC-2012数据集上进行实验。

- LRN没有能改善模型。
- 分类错误率随深度增加而下降。
- $1 \times 1$不如$3 \times 3$。
- 小滤波器的深度网络优于大滤波器的浅层网络。