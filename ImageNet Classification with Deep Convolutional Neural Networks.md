## 引言

## 数据集

缩放图像到$256 \times 256$，保留原始RGB通道。

## 架构

### ReLU

避免Tanh或者Sigmod这样的饱和非线性函数运算速度较慢且存在梯度消失问题。ReLU收敛较快。

### 多GPU

双GPU，某些层只在单个GPU上放一半的神经元，下一层的神经元只连接和自己处于同一个GPU的上一层神经元。

![](./images/AlexNetMultiGPU.drawio.svg)

### 局部响应归一化LRN

$$b_{x,y}^i = a_{x,y}^i / (k + \alpha \sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)} (a_{x,y}^j)^2)^ \beta$$
其实就是将相邻$N$个神经元的对应位置$(x,y)$的响应输出进行归一化。这样可以在不同核的响应输出之间产生竞争，防止依赖单个神经元，提高泛化能力。
（LRN在现代神经网络中已经很少使用）

![](./images/AlexNetLRN.drawio.svg)

### 重叠池化

其实就是size = 3，stride = 2的池化层，相邻两个池化单元会有一个重叠。错误率更低，原因玄学。

## 网络结构

总共八层，五个卷积层加三个全连接层：
1. 卷积层：$96$个核，输入$224 \times 224 \times 3$，大小$11 \times 11 \times 3$，步长$4$，输出$55 \times 55 \times 96$。
2. 归一，池化：大小$3 \times 3$，步长2，输出$27 \times 27 \times 96$。
3. 卷积层：$256$个核，大小$5 \times 5 \times 48$（只和同GPU的核相连），输出$27 \times 27 \times 256$。
4. 归一，池化：大小$3 \times 3$，步长2，输出$13 \times 13 \times 256$。
5. 卷积层：$384$个核，大小$3 \times 3 \times 256$，输出$13 \times 13 \times 384$。
6. 卷积层：$384$个核，大小$3 \times 3 \times 192$（只和同GPU的核相连），输出$13 \times 13 \times 384$。
7. 卷积层：$256$个核，大小$3 \times 3 \times 192$，输出$13 \times 13 \times 256$。
8. 全连接层：$4096$个神经元。
9. 全连接层：$4096$个神经元。
10. 全连接层：$1000$个神经元。

参数数量：$$\begin{aligned}11\times11\times3\times96+5\times5\times48\times256+3\times3\times256\times384+3\times3\times192\times384+ \\ 3\times3\times192\times256+13\times13\times256\times4096+4096\times4096+4096\times1000\end{aligned}$$
## 优化

### 数据增强

图像平移和水平反射。

对训练集中的RGB进行主成分分析（PAC）并将主成分倍数以一定规则加到像素值中。

### Dropout

0.5的概率进行Dropout，提高泛化能力，防止过拟合。预测时对神经元输出乘以0.5。

### 学习率

BatchSize = 128，动量0.9，衰减0.0005。

### 参数初始化

用均值为0，标准差为0.01的高斯分布对权重进行初始化，第二第四第五个卷积层和全连接层偏置初始化为1，其余初始化为0。
因为用$N~(0,0.01)$初始化权重后，权重集中在0附近，大量神经元不活跃，加上偏置项1后，可以更快激活神经元（梯度传播更有效率），加速收敛。