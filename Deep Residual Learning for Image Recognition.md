## 1 介绍

随着网络层数增加，出现梯度消失和梯度爆炸问题。

> 梯度消失：梯度的逐层叠加使得梯度逐渐进入饱和函数的饱和区间或者ReLU函数的负半轴。网络过深导致在复合求导时连续乘以小于1的梯度。
> 梯度爆炸：损失值随着层数增加在ReLU的正半轴叠加。权重过大导致梯度过大，因为输入的梯度一般和权重正相关。

解决办法是参数的归一初始化以及增加归一化层，这个可以解决因为权重参数过大导致的梯度爆炸问题。

另一个问题是随着层数增加准确度不再上升，甚至出现下降。

本文引出深度残差学习来解决退化问题。这种结构可以通过在前馈网络中增加快捷连接实现。

这种连接不增加额外的参数和计算负责度。

实验表明可以这种网络可以大幅增加深度从而提升准确度。

## 2 相关工作

早期MLP中已经开始采用类似的残差设计。

## 3 深度残差学习

### 3.1 残差学习

将$H(x)$视为原本没有残差网络时，一系列非线性层堆叠产生的映射，$x$是堆叠的输入。如果这个堆叠产生的映射可以逼近复杂函数，那么也等价于他们可以逼近残差函数$H(x)-x$。那么这个堆叠只需要等价于映射$F(x) = H(x) - x$就行。

>其实就是说明这种残差设计的正确性。

退化问题表明，求解器难以通过多个非线性层来近似恒等映射，传统非线性层叠加难以学习恒等映射。残差网络可以通过将多个非线性层的权重置零来学习恒等映射。

> 这里说明了残差设计的合理性。如果需要一个恒等映射，不同于传统网络用非线性变换强行拟合，ResNet通过将非线性层权重置零，并将输入x旁路到输出端。这样做也能减少网络的误差，因为非线性层就是难以拟合恒等映射的。

### 3.2 通过捷径实现恒等映射

也可以旁路一个线性映射$y = F(x) + Wx$。但实验表明恒等映射足矣。

### 3.3 网络架构

### 3.4 实现

和VGGNet类似的数据预处理。卷积层和激活层之间添加BN层。

批大小256的随机梯度下降，学习率0.1开始，每次趋于平稳就除以10。权重衰减0.0001，动量0.9。不使用dropout。

## 4 实验

传统网络在34层时出现退化，但并没有发现梯度消失。ResNet在34层时仍然保持较高的准确率。